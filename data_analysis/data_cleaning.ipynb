{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Notebook\n",
    "\n",
    "This notebook processes data obtained from various sources, including the Cochrane Library, Pfizer documents, non-plain examples from the Clinical Trials API for the Pfizer documents, and files from Trial Summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets\n",
    "\n",
    "We have four main datasets divided into training and testing sets, each containing plain (pls) and non-plain (non_pls) texts. These datasets are loaded from CSV files for further processing and analysis. The data has undergone data augmentation by paragraphs and has been processed through an API to obtain linguistic features.\n",
    "\n",
    "- **Dataset 1 (Cochrane Library)**:\n",
    "  - `data_cochrane_train_non_pls`: Non-plain language training set.\n",
    "  - `data_cochrane_train_pls`: Plain language training set.\n",
    "  - `data_cochrane_test_non_pls`: Non-plain language testing set.\n",
    "  - `data_cochrane_test_pls`: Plain language testing set.\n",
    "\n",
    "- **Dataset 2 (Pfizer and Clinical Trials)**:\n",
    "  - `data_pfizer_train_non_pls`: Non-plain language training set.\n",
    "  - `data_pfizer_train_pls`: Plain language training set.\n",
    "  - `data_pfizer_test_non_pls`: Non-plain language testing set.\n",
    "  - `data_pfizer_test_pls`: Plain language testing set.\n",
    "\n",
    "- **Dataset 3 (Trial Summaries)**:\n",
    "  - `data_trial_summaries_test_pls`: Plain language testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset 1 (Cochrane Library) training and testing sets\n",
    "data_cochrane_train_non_pls = pd.read_csv('data/data_cochrane_train_non_pls.csv')\n",
    "data_cochrane_train_pls = pd.read_csv('data/data_cochrane_train_pls.csv')\n",
    "data_cochrane_test_non_pls = pd.read_csv('data/data_cochrane_test_non_pls.csv')\n",
    "data_cochrane_test_pls = pd.read_csv('data/data_cochrane_test_pls.csv')\n",
    "\n",
    "# Load Dataset 2 (Pfizer and Clinical Trials) training and testing sets\n",
    "data_pfizer_train_non_pls = pd.read_csv('data/data_pfizer_train_non_pls.csv')\n",
    "data_pfizer_train_pls = pd.read_csv('data/data_pfizer_train_pls.csv')\n",
    "data_pfizer_test_non_pls = pd.read_csv('data/data_pfizer_test_non_pls.csv')\n",
    "data_pfizer_test_pls = pd.read_csv('data/data_pfizer_test_pls.csv')\n",
    "\n",
    "# Load Dataset 3 (Trial Summaries) only test set\n",
    "data_trial_summaries_test_pls = pd.read_csv('data/data_trial_summaries_test_pls.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Labeling Data\n",
    "\n",
    "To facilitate the identification and classification of plain and non-plain texts, we add a `label` column to each dataset:\n",
    "- A label of `0` is assigned to non-plain texts.\n",
    "- A label of `1` is assigned to plain texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a label column to Dataset 1 (Cochrane Library) training sets\n",
    "data_cochrane_train_non_pls['label'] = 0\n",
    "data_cochrane_train_pls['label'] = 1\n",
    "\n",
    "# Add a label column to Dataset 2 (Pfizer and Clinical Trials) training sets\n",
    "data_pfizer_train_non_pls['label'] = 0\n",
    "data_pfizer_train_pls['label'] = 1\n",
    "\n",
    "# Add a label column to Dataset 1 (Cochrane Library) testing sets\n",
    "data_cochrane_test_non_pls['label'] = 0\n",
    "data_cochrane_test_pls['label'] = 1\n",
    "\n",
    "# Add a label column to Dataset 2 (Pfizer and Clinical Trials) testing sets\n",
    "data_pfizer_test_non_pls['label'] = 0\n",
    "data_pfizer_test_pls['label'] = 1\n",
    "\n",
    "# Add a label column to Dataset 3 (TS)\n",
    "data_trial_summaries_test_pls['label'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining and Shuffling Data\n",
    "\n",
    "To create comprehensive training and testing datasets, we concatenate the individual datasets (both plain and non-plain texts) into single DataFrames. This helps ensure that the machine learning models have a diverse and representative set of examples to learn from.\n",
    "\n",
    "After concatenating, we shuffle the data to remove any order bias and ensure a random distribution of samples. On the other hand, the data from TS remains separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate training data from both datasets\n",
    "data_train = pd.concat([data_cochrane_train_non_pls, data_cochrane_train_pls, data_pfizer_train_non_pls, data_pfizer_train_pls])\n",
    "\n",
    "# Concatenate testing data from both datasets\n",
    "data_test = pd.concat([data_cochrane_test_non_pls, data_cochrane_test_pls, data_pfizer_test_non_pls, data_pfizer_test_pls])\n",
    "\n",
    "# Shuffle the training data to ensure random distribution\n",
    "data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Shuffle the testing data to ensure random distribution\n",
    "data_test = data_test.sample(frac=1).reset_index(drop=True)\n",
    "data_trial_summaries_test_pls = data_trial_summaries_test_pls.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the \"histograms\" folder exists\n",
    "if not os.path.exists(\"histograms\"):\n",
    "    os.makedirs(\"histograms\")\n",
    "\n",
    "# Concatenate training data from both datasets\n",
    "data_train = pd.concat([data_cochrane_train_non_pls, data_cochrane_train_pls])\n",
    "\n",
    "# Concatenate testing data from both datasets\n",
    "data_test = pd.concat([data_cochrane_test_non_pls, data_cochrane_test_pls])\n",
    "\n",
    "# Merge train and test data\n",
    "data = pd.concat([data_train, data_test])\n",
    "\n",
    "data_plain = data[data['label'] == 1]\n",
    "data_no_plain = data[data['label'] == 0] \n",
    "\n",
    "# Map the labels to their names\n",
    "label_mapping = {0: 'Technical', 1: 'Plain'}\n",
    "data['label'] = data['label'].map(label_mapping)\n",
    "\n",
    "# Columns to plot\n",
    "columns_to_plot = ['Kincaid', 'ARI', 'Coleman-Liau', 'FleschReadingEase', 'GunningFogIndex', 'LIX', 'SMOGIndex', 'RIX', \n",
    "                   'DaleChallIndex', 'total_words', 'total_sentences', 'total_characters', 'passive_voice', 'active_voice', \n",
    "                   'passive_toks', 'active_toks', 'verbs', 'nouns', 'adjectives', 'adverbs', 'prepositions', 'auxiliaries', \n",
    "                   'conjunctions', 'coord_conjunctions', 'determiners', 'interjections', 'numbers', 'particles', 'pronouns', \n",
    "                   'proper_nouns', 'punctuations', 'subordinating_conjunctions', 'symbols', 'other', 'money', 'persons', \n",
    "                   'norp', 'facilities', 'organizations', 'gpe', 'products', 'events', 'works', 'languages', 'dates', 'times', \n",
    "                   'quantities', 'ordinals', 'cardinals', 'percentages', 'stopwords', 'locations', 'laws', 'characters_per_word', \n",
    "                   'syll_per_word', 'words_per_sentence', 'sentences_per_paragraph', 'type_token_ratio', 'characters', 'syllables', \n",
    "                   'words', 'wordtypes', 'sentences', 'paragraphs', 'long_words', 'complex_words', 'complex_words_dc', 'tobeverb', \n",
    "                   'auxverb', 'conjunction', 'nominalization']\n",
    "\n",
    "# Generate histograms\n",
    "for column in columns_to_plot:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.histplot(data=data, x=column, hue='label', bins=80, element='bars', edgecolor=None, stat='percent')\n",
    "    plt.title(f'Histogram of {column}')\n",
    "    plt.legend(title='Label', labels=['Technical', 'Plain'])\n",
    "    plt.ylabel('Percentage of texts (%)')\n",
    "    plt.savefig(f'histograms/{column}.tiff', format='tiff')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping columns\n",
    "### Dropping Zero-Value columns\n",
    "We identify and remove columns that contain only zero values to streamline the datasets.\n",
    "\n",
    "1. Identify columns with only zero values.\n",
    "2. Drop these columns from the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['conjunctions'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Identify columns in the training data that contain only zero values\n",
    "zeros = data_train.columns[(data_train == 0).all()]\n",
    "print(zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns With a Small Non-Zero Percentage\n",
    "\n",
    "We calculate the percentage of non-zero values for each column in the training dataset. This helps in understanding the distribution of values and identifying columns that might have a significant amount of zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kincaid</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARI</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coleman-Liau</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FleschReadingEase</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tobeverb</th>\n",
       "      <td>99.971117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auxverb</th>\n",
       "      <td>82.705123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conjunction</th>\n",
       "      <td>99.995874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nominalization</th>\n",
       "      <td>99.971117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>34.517547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0\n",
       "file               100.000000\n",
       "Kincaid            100.000000\n",
       "ARI                100.000000\n",
       "Coleman-Liau       100.000000\n",
       "FleschReadingEase  100.000000\n",
       "...                       ...\n",
       "tobeverb            99.971117\n",
       "auxverb             82.705123\n",
       "conjunction         99.995874\n",
       "nominalization      99.971117\n",
       "label               34.517547\n",
       "\n",
       "[73 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (data_train != 0)\n",
    "percent_zeros = mask.sum() / mask.count() * 100\n",
    "percent_zeros.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set a threshold of 3% to identify columns with a low percentage of non-zero values. Columns below this threshold will be listed for removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['conjunctions', 'events', 'languages'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a threshold for the minimum acceptable percentage of non-zero values\n",
    "threshold = 3\n",
    "\n",
    "# Identify columns that fall below the threshold\n",
    "columns_to_drop = percent_zeros[percent_zeros < threshold].index\n",
    "\n",
    "# List the columns to be dropped\n",
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Redundant Columns\n",
    "\n",
    "We identify some columns that are unnecessary since their information is already found in others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to columns to drop\n",
    "columns_to_drop = columns_to_drop.append(pd.Index(['characters', 'sentences', 'words', 'paragraphs']))\n",
    "columns_to_drop = columns_to_drop.append(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['conjunctions', 'events', 'languages', 'characters', 'sentences',\n",
       "       'words', 'paragraphs', 'conjunctions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns With Not Enough _p_-value\n",
    "\n",
    "Taking into account the hypothesis tests carried out, we eliminated the columns with a low p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['conjunctions', 'events', 'languages', 'characters', 'sentences',\n",
       "       'words', 'paragraphs', 'conjunctions', 'money', 'passive_voice',\n",
       "       'interjections', 'facilities'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = columns_to_drop.append(pd.Index(['money', 'passive_voice', 'interjections', 'facilities']))\n",
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns with low non-zero percentage from the training and testing dataset\n",
    "data_train = data_train.drop(columns_to_drop, axis=1)\n",
    "data_test = data_test.drop(columns_to_drop, axis=1)\n",
    "data_trial_summaries_test_pls = data_trial_summaries_test_pls.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define our feature set (`X`) and label set (`y`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train.drop(['label'], axis=1)\n",
    "y_train = data_train['label']\n",
    "\n",
    "X_test = data_test.drop(['label'], axis=1)\n",
    "y_test = data_test['label']\n",
    "\n",
    "X_ts = data_trial_summaries_test_pls.drop(['label'], axis=1)\n",
    "y_ts = data_trial_summaries_test_pls['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of Data\n",
    "\n",
    "In this section, we normalize the data to ensure that all features are on a comparable scale. We have identified that certain columns require a specific normalization method based on min-max normalization, while other columns will be normalized by the count of words of their respective type.\n",
    "\n",
    "### Special Columns\n",
    "\n",
    "The following columns require min-max normalization:\n",
    "\n",
    "- `total_words`\n",
    "- `total_sentences`\n",
    "- `total_characters`\n",
    "- `characters_per_word`\n",
    "- `syll_per_word`\n",
    "- `words_per_sentence`\n",
    "- `sentences_per_paragraph`\n",
    "- `type_token_ratio`\n",
    "- `syllables`\n",
    "- `wordtypes`\n",
    "- `Kincaid`\n",
    "- `ARI`\n",
    "- `Coleman-Liau`\n",
    "- `FleschReadingEase`\n",
    "- `GunningFogIndex`\n",
    "- `LIX`\n",
    "- `SMOGIndex`\n",
    "- `RIX`\n",
    "- `DaleChallIndex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that require min-max normalization\n",
    "special_columns = ['total_words', 'total_sentences', 'total_characters',                 \n",
    "                   'characters_per_word','syll_per_word', 'words_per_sentence', 'sentences_per_paragraph',\n",
    "                   'type_token_ratio', 'syllables', 'wordtypes', 'Kincaid', 'ARI', 'Coleman-Liau', 'FleschReadingEase',\n",
    "                   'GunningFogIndex', 'LIX', 'SMOGIndex', 'RIX', 'DaleChallIndex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns to ignore and we don't want to normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_ignore = ['file', 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the columns not listed in the special columns by dividing their values by the `total_words` column. This ensures these features are scaled appropriately relative to the total word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize columns not listed in special columns by dividing by total_words\n",
    "for column in X_train.columns:\n",
    "    if column not in special_columns:\n",
    "        if column in cols_to_ignore:\n",
    "            continue\n",
    "        X_train[column] = X_train[column] / X_train['total_words']\n",
    "        X_test[column] = X_test[column] / X_test['total_words']\n",
    "        X_ts[column] = X_ts[column] / X_ts['total_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max Normalization for Special Columns\n",
    "\n",
    "For columns identified as requiring min-max normalization, we scale their values to a range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store the minimum and maximum values for each special column\n",
    "maximos = {}\n",
    "minimos = {}\n",
    "\n",
    "# Perform min-max normalization for each special column\n",
    "for column in special_columns:\n",
    "    if column in cols_to_ignore:\n",
    "        continue\n",
    "    # Calculate the minimum and maximum values across training and testing datasets\n",
    "    minimum = min(X_train[column].min(), X_test[column].min())\n",
    "    maximum = max(X_train[column].max(), X_test[column].max())\n",
    "    \n",
    "    # Store the calculated minimum and maximum values\n",
    "    maximos[str(column)] = maximum\n",
    "    minimos[str(column)] = minimum\n",
    "\n",
    "    # Apply min-max normalization to the training and testing datasets\n",
    "    X_train[column] = (X_train[column] - minimum) / (maximum - minimum)\n",
    "    X_test[column] = (X_test[column] - minimum) / (maximum - minimum)\n",
    "    X_ts[column] = (X_ts[column] - minimum) / (maximum - minimum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Normalized Data\n",
    "\n",
    "In this section, we combine the normalized training and testing datasets back into a single DataFrame for further analysis. We also reattach the labels to the combined dataset.\n",
    "\n",
    "### Steps:\n",
    "1. Concatenate the normalized training and testing feature sets.\n",
    "2. Reattach the labels to the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the normalized training and testing feature sets into a single DataFrame\n",
    "data = pd.concat([X_train, X_test])\n",
    "\n",
    "# Reattach the labels to the combined dataset\n",
    "data['label'] = pd.concat([y_train, y_test])\n",
    "\n",
    "# Reattach the labels to the combined dataset for TS dataset.\n",
    "data_ts = pd.concat([X_ts])\n",
    "data_ts['label'] = y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>Kincaid</th>\n",
       "      <th>ARI</th>\n",
       "      <th>Coleman-Liau</th>\n",
       "      <th>FleschReadingEase</th>\n",
       "      <th>GunningFogIndex</th>\n",
       "      <th>LIX</th>\n",
       "      <th>SMOGIndex</th>\n",
       "      <th>RIX</th>\n",
       "      <th>DaleChallIndex</th>\n",
       "      <th>...</th>\n",
       "      <th>syllables</th>\n",
       "      <th>wordtypes</th>\n",
       "      <th>long_words</th>\n",
       "      <th>complex_words</th>\n",
       "      <th>complex_words_dc</th>\n",
       "      <th>tobeverb</th>\n",
       "      <th>auxverb</th>\n",
       "      <th>conjunction</th>\n",
       "      <th>nominalization</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002-14651858.CD003237.pub2-abstract.txt</td>\n",
       "      <td>0.036417</td>\n",
       "      <td>0.027587</td>\n",
       "      <td>0.738893</td>\n",
       "      <td>0.921574</td>\n",
       "      <td>0.033140</td>\n",
       "      <td>0.041748</td>\n",
       "      <td>0.104468</td>\n",
       "      <td>0.028943</td>\n",
       "      <td>0.110047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065053</td>\n",
       "      <td>0.236364</td>\n",
       "      <td>0.393678</td>\n",
       "      <td>0.244253</td>\n",
       "      <td>0.554598</td>\n",
       "      <td>0.048851</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.054598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002-14651858.CD011070.pub2-abstract.txt_ac...</td>\n",
       "      <td>0.058573</td>\n",
       "      <td>0.054424</td>\n",
       "      <td>0.670448</td>\n",
       "      <td>0.915326</td>\n",
       "      <td>0.061864</td>\n",
       "      <td>0.067161</td>\n",
       "      <td>0.176646</td>\n",
       "      <td>0.060099</td>\n",
       "      <td>0.138720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112028</td>\n",
       "      <td>0.392727</td>\n",
       "      <td>0.365159</td>\n",
       "      <td>0.236181</td>\n",
       "      <td>0.584590</td>\n",
       "      <td>0.028476</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.043551</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002-14651858.CD008856.pub2-pls.txt_accumul...</td>\n",
       "      <td>0.049334</td>\n",
       "      <td>0.043075</td>\n",
       "      <td>0.688413</td>\n",
       "      <td>0.918900</td>\n",
       "      <td>0.045598</td>\n",
       "      <td>0.051959</td>\n",
       "      <td>0.131758</td>\n",
       "      <td>0.042423</td>\n",
       "      <td>0.061509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136797</td>\n",
       "      <td>0.338182</td>\n",
       "      <td>0.328058</td>\n",
       "      <td>0.195683</td>\n",
       "      <td>0.366906</td>\n",
       "      <td>0.034532</td>\n",
       "      <td>0.017266</td>\n",
       "      <td>0.035971</td>\n",
       "      <td>0.050360</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1002-14651858.MR000043.pub2-abstract.txt_ac...</td>\n",
       "      <td>0.028315</td>\n",
       "      <td>0.024618</td>\n",
       "      <td>0.559319</td>\n",
       "      <td>0.954066</td>\n",
       "      <td>0.033862</td>\n",
       "      <td>0.034057</td>\n",
       "      <td>0.104649</td>\n",
       "      <td>0.024164</td>\n",
       "      <td>0.134249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048826</td>\n",
       "      <td>0.185455</td>\n",
       "      <td>0.287324</td>\n",
       "      <td>0.205634</td>\n",
       "      <td>0.616901</td>\n",
       "      <td>0.022535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039437</td>\n",
       "      <td>0.039437</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1002-14651858.CD011507.pub2-pls.txt_accumul...</td>\n",
       "      <td>0.058157</td>\n",
       "      <td>0.047716</td>\n",
       "      <td>0.590786</td>\n",
       "      <td>0.911095</td>\n",
       "      <td>0.056511</td>\n",
       "      <td>0.053864</td>\n",
       "      <td>0.159376</td>\n",
       "      <td>0.041305</td>\n",
       "      <td>0.065922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078007</td>\n",
       "      <td>0.249091</td>\n",
       "      <td>0.273148</td>\n",
       "      <td>0.212963</td>\n",
       "      <td>0.363426</td>\n",
       "      <td>0.050926</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.030093</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11844</th>\n",
       "      <td>10.1002-14651858.CD011511.pub2-abstract.txt_ac...</td>\n",
       "      <td>0.055242</td>\n",
       "      <td>0.044445</td>\n",
       "      <td>0.644924</td>\n",
       "      <td>0.908730</td>\n",
       "      <td>0.055324</td>\n",
       "      <td>0.056145</td>\n",
       "      <td>0.166429</td>\n",
       "      <td>0.047088</td>\n",
       "      <td>0.135721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103345</td>\n",
       "      <td>0.310909</td>\n",
       "      <td>0.350598</td>\n",
       "      <td>0.258964</td>\n",
       "      <td>0.605578</td>\n",
       "      <td>0.009960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039841</td>\n",
       "      <td>0.049801</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11845</th>\n",
       "      <td>10.1002-14651858.CD010019.pub4-pls.txt_accumul...</td>\n",
       "      <td>0.047232</td>\n",
       "      <td>0.041954</td>\n",
       "      <td>0.562071</td>\n",
       "      <td>0.933279</td>\n",
       "      <td>0.043064</td>\n",
       "      <td>0.046814</td>\n",
       "      <td>0.105070</td>\n",
       "      <td>0.033870</td>\n",
       "      <td>0.040756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060214</td>\n",
       "      <td>0.194545</td>\n",
       "      <td>0.253165</td>\n",
       "      <td>0.139241</td>\n",
       "      <td>0.301266</td>\n",
       "      <td>0.043038</td>\n",
       "      <td>0.010127</td>\n",
       "      <td>0.045570</td>\n",
       "      <td>0.035443</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11846</th>\n",
       "      <td>10.1002-14651858.CD011426.pub2-abstract.txt_ac...</td>\n",
       "      <td>0.046779</td>\n",
       "      <td>0.037102</td>\n",
       "      <td>0.753543</td>\n",
       "      <td>0.909529</td>\n",
       "      <td>0.043558</td>\n",
       "      <td>0.052363</td>\n",
       "      <td>0.136434</td>\n",
       "      <td>0.042220</td>\n",
       "      <td>0.113444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095089</td>\n",
       "      <td>0.290909</td>\n",
       "      <td>0.413717</td>\n",
       "      <td>0.258850</td>\n",
       "      <td>0.553097</td>\n",
       "      <td>0.028761</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.035398</td>\n",
       "      <td>0.035398</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11847</th>\n",
       "      <td>10.1002-14651858.CD004378.pub3-abstract.txt_ac...</td>\n",
       "      <td>0.042606</td>\n",
       "      <td>0.036770</td>\n",
       "      <td>0.590955</td>\n",
       "      <td>0.934243</td>\n",
       "      <td>0.041240</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>0.116300</td>\n",
       "      <td>0.037639</td>\n",
       "      <td>0.107453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110036</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.301711</td>\n",
       "      <td>0.171073</td>\n",
       "      <td>0.496112</td>\n",
       "      <td>0.015552</td>\n",
       "      <td>0.009331</td>\n",
       "      <td>0.023328</td>\n",
       "      <td>0.038880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11848</th>\n",
       "      <td>10.1002-14651858.CD006931.pub2-abstract.txt</td>\n",
       "      <td>0.030175</td>\n",
       "      <td>0.024254</td>\n",
       "      <td>0.598668</td>\n",
       "      <td>0.944832</td>\n",
       "      <td>0.035368</td>\n",
       "      <td>0.035030</td>\n",
       "      <td>0.111359</td>\n",
       "      <td>0.024783</td>\n",
       "      <td>0.101096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067046</td>\n",
       "      <td>0.247273</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.235577</td>\n",
       "      <td>0.521635</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.040865</td>\n",
       "      <td>0.045673</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60320 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    file   Kincaid       ARI  \\\n",
       "0            10.1002-14651858.CD003237.pub2-abstract.txt  0.036417  0.027587   \n",
       "1      10.1002-14651858.CD011070.pub2-abstract.txt_ac...  0.058573  0.054424   \n",
       "2      10.1002-14651858.CD008856.pub2-pls.txt_accumul...  0.049334  0.043075   \n",
       "3      10.1002-14651858.MR000043.pub2-abstract.txt_ac...  0.028315  0.024618   \n",
       "4      10.1002-14651858.CD011507.pub2-pls.txt_accumul...  0.058157  0.047716   \n",
       "...                                                  ...       ...       ...   \n",
       "11844  10.1002-14651858.CD011511.pub2-abstract.txt_ac...  0.055242  0.044445   \n",
       "11845  10.1002-14651858.CD010019.pub4-pls.txt_accumul...  0.047232  0.041954   \n",
       "11846  10.1002-14651858.CD011426.pub2-abstract.txt_ac...  0.046779  0.037102   \n",
       "11847  10.1002-14651858.CD004378.pub3-abstract.txt_ac...  0.042606  0.036770   \n",
       "11848        10.1002-14651858.CD006931.pub2-abstract.txt  0.030175  0.024254   \n",
       "\n",
       "       Coleman-Liau  FleschReadingEase  GunningFogIndex       LIX  SMOGIndex  \\\n",
       "0          0.738893           0.921574         0.033140  0.041748   0.104468   \n",
       "1          0.670448           0.915326         0.061864  0.067161   0.176646   \n",
       "2          0.688413           0.918900         0.045598  0.051959   0.131758   \n",
       "3          0.559319           0.954066         0.033862  0.034057   0.104649   \n",
       "4          0.590786           0.911095         0.056511  0.053864   0.159376   \n",
       "...             ...                ...              ...       ...        ...   \n",
       "11844      0.644924           0.908730         0.055324  0.056145   0.166429   \n",
       "11845      0.562071           0.933279         0.043064  0.046814   0.105070   \n",
       "11846      0.753543           0.909529         0.043558  0.052363   0.136434   \n",
       "11847      0.590955           0.934243         0.041240  0.047766   0.116300   \n",
       "11848      0.598668           0.944832         0.035368  0.035030   0.111359   \n",
       "\n",
       "            RIX  DaleChallIndex  ...  syllables  wordtypes  long_words  \\\n",
       "0      0.028943        0.110047  ...   0.065053   0.236364    0.393678   \n",
       "1      0.060099        0.138720  ...   0.112028   0.392727    0.365159   \n",
       "2      0.042423        0.061509  ...   0.136797   0.338182    0.328058   \n",
       "3      0.024164        0.134249  ...   0.048826   0.185455    0.287324   \n",
       "4      0.041305        0.065922  ...   0.078007   0.249091    0.273148   \n",
       "...         ...             ...  ...        ...        ...         ...   \n",
       "11844  0.047088        0.135721  ...   0.103345   0.310909    0.350598   \n",
       "11845  0.033870        0.040756  ...   0.060214   0.194545    0.253165   \n",
       "11846  0.042220        0.113444  ...   0.095089   0.290909    0.413717   \n",
       "11847  0.037639        0.107453  ...   0.110036   0.300000    0.301711   \n",
       "11848  0.024783        0.101096  ...   0.067046   0.247273    0.312500   \n",
       "\n",
       "       complex_words  complex_words_dc  tobeverb   auxverb  conjunction  \\\n",
       "0           0.244253          0.554598  0.048851  0.005747     0.051724   \n",
       "1           0.236181          0.584590  0.028476  0.006700     0.043551   \n",
       "2           0.195683          0.366906  0.034532  0.017266     0.035971   \n",
       "3           0.205634          0.616901  0.022535  0.000000     0.039437   \n",
       "4           0.212963          0.363426  0.050926  0.006944     0.030093   \n",
       "...              ...               ...       ...       ...          ...   \n",
       "11844       0.258964          0.605578  0.009960  0.000000     0.039841   \n",
       "11845       0.139241          0.301266  0.043038  0.010127     0.045570   \n",
       "11846       0.258850          0.553097  0.028761  0.008850     0.035398   \n",
       "11847       0.171073          0.496112  0.015552  0.009331     0.023328   \n",
       "11848       0.235577          0.521635  0.031250  0.004808     0.040865   \n",
       "\n",
       "       nominalization  label  \n",
       "0            0.054598      0  \n",
       "1            0.050251      0  \n",
       "2            0.050360      1  \n",
       "3            0.039437      0  \n",
       "4            0.034722      1  \n",
       "...               ...    ...  \n",
       "11844        0.049801      0  \n",
       "11845        0.035443      1  \n",
       "11846        0.035398      0  \n",
       "11847        0.038880      0  \n",
       "11848        0.045673      0  \n",
       "\n",
       "[60320 rows x 62 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>Kincaid</th>\n",
       "      <th>ARI</th>\n",
       "      <th>Coleman-Liau</th>\n",
       "      <th>FleschReadingEase</th>\n",
       "      <th>GunningFogIndex</th>\n",
       "      <th>LIX</th>\n",
       "      <th>SMOGIndex</th>\n",
       "      <th>RIX</th>\n",
       "      <th>DaleChallIndex</th>\n",
       "      <th>...</th>\n",
       "      <th>syllables</th>\n",
       "      <th>wordtypes</th>\n",
       "      <th>long_words</th>\n",
       "      <th>complex_words</th>\n",
       "      <th>complex_words_dc</th>\n",
       "      <th>tobeverb</th>\n",
       "      <th>auxverb</th>\n",
       "      <th>conjunction</th>\n",
       "      <th>nominalization</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11056-MT_MED_9011_page12.txt</td>\n",
       "      <td>0.009082</td>\n",
       "      <td>0.008457</td>\n",
       "      <td>0.632308</td>\n",
       "      <td>0.964573</td>\n",
       "      <td>-0.002733</td>\n",
       "      <td>0.009854</td>\n",
       "      <td>-0.009014</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.038490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035730</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.252525</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.350168</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.033670</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1252-MT_MED_9011_page4.txt</td>\n",
       "      <td>0.025508</td>\n",
       "      <td>0.020824</td>\n",
       "      <td>0.575991</td>\n",
       "      <td>0.953215</td>\n",
       "      <td>0.019137</td>\n",
       "      <td>0.025435</td>\n",
       "      <td>0.053206</td>\n",
       "      <td>0.016729</td>\n",
       "      <td>0.034890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041281</td>\n",
       "      <td>0.127273</td>\n",
       "      <td>0.253247</td>\n",
       "      <td>0.123377</td>\n",
       "      <td>0.331169</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1017-MT_MED_9011_page8.txt</td>\n",
       "      <td>0.041861</td>\n",
       "      <td>0.036483</td>\n",
       "      <td>0.710361</td>\n",
       "      <td>0.925649</td>\n",
       "      <td>0.036914</td>\n",
       "      <td>0.050692</td>\n",
       "      <td>0.109818</td>\n",
       "      <td>0.040892</td>\n",
       "      <td>0.072429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033310</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.385593</td>\n",
       "      <td>0.190678</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11555-MT_MED_9011_page8.txt</td>\n",
       "      <td>0.018527</td>\n",
       "      <td>0.017148</td>\n",
       "      <td>0.714199</td>\n",
       "      <td>0.952129</td>\n",
       "      <td>0.006668</td>\n",
       "      <td>0.020163</td>\n",
       "      <td>0.020414</td>\n",
       "      <td>0.010161</td>\n",
       "      <td>0.046016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030320</td>\n",
       "      <td>0.123636</td>\n",
       "      <td>0.286290</td>\n",
       "      <td>0.108871</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.012097</td>\n",
       "      <td>0.028226</td>\n",
       "      <td>0.040323</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11638-MT_MED_9011_page5.txt</td>\n",
       "      <td>0.028544</td>\n",
       "      <td>0.021762</td>\n",
       "      <td>0.580259</td>\n",
       "      <td>0.946244</td>\n",
       "      <td>0.022391</td>\n",
       "      <td>0.023072</td>\n",
       "      <td>0.065498</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>0.013188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028897</td>\n",
       "      <td>0.030909</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.141667</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.029167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>1092-MT_MED_9011_page1.txt</td>\n",
       "      <td>0.019466</td>\n",
       "      <td>0.013107</td>\n",
       "      <td>0.521313</td>\n",
       "      <td>0.959827</td>\n",
       "      <td>0.010495</td>\n",
       "      <td>0.016049</td>\n",
       "      <td>0.027310</td>\n",
       "      <td>0.009707</td>\n",
       "      <td>0.024140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053950</td>\n",
       "      <td>0.167273</td>\n",
       "      <td>0.220159</td>\n",
       "      <td>0.098143</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.053050</td>\n",
       "      <td>0.007958</td>\n",
       "      <td>0.031830</td>\n",
       "      <td>0.005305</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>11547-MT_MED_9011_page3.txt</td>\n",
       "      <td>0.019441</td>\n",
       "      <td>0.011888</td>\n",
       "      <td>0.407538</td>\n",
       "      <td>0.967844</td>\n",
       "      <td>0.011071</td>\n",
       "      <td>0.006432</td>\n",
       "      <td>0.016621</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>0.045562</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007687</td>\n",
       "      <td>-0.040000</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.369863</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>11230-MT_MED_9011_page5.txt</td>\n",
       "      <td>0.023610</td>\n",
       "      <td>0.016993</td>\n",
       "      <td>0.567945</td>\n",
       "      <td>0.951788</td>\n",
       "      <td>0.020843</td>\n",
       "      <td>0.024448</td>\n",
       "      <td>0.064962</td>\n",
       "      <td>0.015489</td>\n",
       "      <td>0.022852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035160</td>\n",
       "      <td>0.078182</td>\n",
       "      <td>0.270073</td>\n",
       "      <td>0.164234</td>\n",
       "      <td>0.299270</td>\n",
       "      <td>0.054745</td>\n",
       "      <td>0.014599</td>\n",
       "      <td>0.025547</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>11522-MT_MED_9011_page11.txt</td>\n",
       "      <td>0.038504</td>\n",
       "      <td>0.032445</td>\n",
       "      <td>0.623532</td>\n",
       "      <td>0.934907</td>\n",
       "      <td>0.028178</td>\n",
       "      <td>0.036370</td>\n",
       "      <td>0.069718</td>\n",
       "      <td>0.026022</td>\n",
       "      <td>0.041769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015516</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.266272</td>\n",
       "      <td>0.118343</td>\n",
       "      <td>0.331361</td>\n",
       "      <td>0.041420</td>\n",
       "      <td>0.023669</td>\n",
       "      <td>0.035503</td>\n",
       "      <td>0.017751</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>11574-MT_MED_9011_page9.txt</td>\n",
       "      <td>0.019362</td>\n",
       "      <td>0.013736</td>\n",
       "      <td>0.602179</td>\n",
       "      <td>0.953712</td>\n",
       "      <td>0.013586</td>\n",
       "      <td>0.027682</td>\n",
       "      <td>0.042820</td>\n",
       "      <td>0.015156</td>\n",
       "      <td>0.060574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027758</td>\n",
       "      <td>0.047273</td>\n",
       "      <td>0.340517</td>\n",
       "      <td>0.150862</td>\n",
       "      <td>0.422414</td>\n",
       "      <td>0.043103</td>\n",
       "      <td>0.021552</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.025862</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1034 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              file   Kincaid       ARI  Coleman-Liau  \\\n",
       "0     11056-MT_MED_9011_page12.txt  0.009082  0.008457      0.632308   \n",
       "1       1252-MT_MED_9011_page4.txt  0.025508  0.020824      0.575991   \n",
       "2       1017-MT_MED_9011_page8.txt  0.041861  0.036483      0.710361   \n",
       "3      11555-MT_MED_9011_page8.txt  0.018527  0.017148      0.714199   \n",
       "4      11638-MT_MED_9011_page5.txt  0.028544  0.021762      0.580259   \n",
       "...                            ...       ...       ...           ...   \n",
       "1029    1092-MT_MED_9011_page1.txt  0.019466  0.013107      0.521313   \n",
       "1030   11547-MT_MED_9011_page3.txt  0.019441  0.011888      0.407538   \n",
       "1031   11230-MT_MED_9011_page5.txt  0.023610  0.016993      0.567945   \n",
       "1032  11522-MT_MED_9011_page11.txt  0.038504  0.032445      0.623532   \n",
       "1033   11574-MT_MED_9011_page9.txt  0.019362  0.013736      0.602179   \n",
       "\n",
       "      FleschReadingEase  GunningFogIndex       LIX  SMOGIndex       RIX  \\\n",
       "0              0.964573        -0.002733  0.009854  -0.009014  0.001859   \n",
       "1              0.953215         0.019137  0.025435   0.053206  0.016729   \n",
       "2              0.925649         0.036914  0.050692   0.109818  0.040892   \n",
       "3              0.952129         0.006668  0.020163   0.020414  0.010161   \n",
       "4              0.946244         0.022391  0.023072   0.065498  0.014870   \n",
       "...                 ...              ...       ...        ...       ...   \n",
       "1029           0.959827         0.010495  0.016049   0.027310  0.009707   \n",
       "1030           0.967844         0.011071  0.006432   0.016621  0.002478   \n",
       "1031           0.951788         0.020843  0.024448   0.064962  0.015489   \n",
       "1032           0.934907         0.028178  0.036370   0.069718  0.026022   \n",
       "1033           0.953712         0.013586  0.027682   0.042820  0.015156   \n",
       "\n",
       "      DaleChallIndex  ...  syllables  wordtypes  long_words  complex_words  \\\n",
       "0           0.038490  ...   0.035730   0.160000    0.252525       0.090909   \n",
       "1           0.034890  ...   0.041281   0.127273    0.253247       0.123377   \n",
       "2           0.072429  ...   0.033310   0.054545    0.385593       0.190678   \n",
       "3           0.046016  ...   0.030320   0.123636    0.286290       0.108871   \n",
       "4           0.013188  ...   0.028897   0.030909    0.225000       0.141667   \n",
       "...              ...  ...        ...        ...         ...            ...   \n",
       "1029        0.024140  ...   0.053950   0.167273    0.220159       0.098143   \n",
       "1030        0.045562  ...  -0.007687  -0.040000    0.109589       0.068493   \n",
       "1031        0.022852  ...   0.035160   0.078182    0.270073       0.164234   \n",
       "1032        0.041769  ...   0.015516   0.054545    0.266272       0.118343   \n",
       "1033        0.060574  ...   0.027758   0.047273    0.340517       0.150862   \n",
       "\n",
       "      complex_words_dc  tobeverb   auxverb  conjunction  nominalization  label  \n",
       "0             0.350168  0.030303  0.010101     0.033670        0.020202      1  \n",
       "1             0.331169  0.035714  0.012987     0.022727        0.022727      1  \n",
       "2             0.423729  0.046610  0.021186     0.038136        0.029661      1  \n",
       "3             0.370968  0.032258  0.012097     0.028226        0.040323      1  \n",
       "4             0.262500  0.029167  0.000000     0.008333        0.004167      1  \n",
       "...                ...       ...       ...          ...             ...    ...  \n",
       "1029          0.310345  0.053050  0.007958     0.031830        0.005305      1  \n",
       "1030          0.369863  0.041096  0.000000     0.027397        0.000000      1  \n",
       "1031          0.299270  0.054745  0.014599     0.025547        0.021898      1  \n",
       "1032          0.331361  0.041420  0.023669     0.035503        0.017751      1  \n",
       "1033          0.422414  0.043103  0.021552     0.017241        0.025862      1  \n",
       "\n",
       "[1034 rows x 62 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Data\n",
    "\n",
    "After normalizing and combining the data, we save the cleaned and processed datasets into CSV files for future use. This includes saving the training and testing datasets, as well as separating the plain and non-plain test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the training features and labels, then save to data_train.csv\n",
    "data = pd.concat([X_train, y_train], axis=1)\n",
    "data.to_csv('data_clean/data_train.csv', index=False)\n",
    "\n",
    "# Combine the testing features and labels, then save to data_test.csv\n",
    "data = pd.concat([X_test, y_test], axis=1)\n",
    "data.to_csv('data_clean/data_test.csv', index=False)\n",
    "\n",
    "data_ts.to_csv('data_clean/data_ts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the \"histograms\" folder exists\n",
    "if not os.path.exists(\"histograms_normalized\"):\n",
    "    os.makedirs(\"histograms_normalized\")\n",
    "\n",
    "url_test = \"data_clean/data_test.csv\"\n",
    "url_train = \"data_clean/data_train.csv\"\n",
    "\n",
    "data_test = pd.read_csv(url_test)\n",
    "data_train = pd.read_csv(url_train)\n",
    "\n",
    "data = pd.concat([data_train, data_test])\n",
    "data_plain = data[data['label'] == 1]\n",
    "data_no_plain = data[data['label'] == 0] \n",
    "\n",
    "# Map the labels to their names\n",
    "label_mapping = {0: 'Technical', 1: 'Plain'}\n",
    "data['label'] = data['label'].map(label_mapping)\n",
    "\n",
    "# Columns to plot\n",
    "columns_to_plot = data.columns[1:-1]\n",
    "\n",
    "# Generate histograms\n",
    "for column in columns_to_plot:\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    sns.histplot(data=data, x=column, hue='label', bins=50, element='bars', edgecolor=None, stat='percent')\n",
    "    plt.title(f'Histogram of {column}')\n",
    "    plt.legend(title='Label', labels=['Technical', 'Plain'])\n",
    "    plt.ylabel('Percentage of texts (%)')\n",
    "    plt.xlabel(f'{column} normalized')\n",
    "    plt.savefig(f'histograms_normalized/{column}.tiff', format='tiff')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
